---
title: "Analytics Assignment 3: GLM - Multinomial Regression"
author: "Revanth Chowdary Ganga (rg361)"
format: pdf
editor: visual
echo: FALSE
output: FALSE
geometry: margin = 1.0cm
---

```{r, warning=FALSE, message=FALSE}
#Loading Required Packages and Libraries
library(tidyverse)
library(dplyr)
library(ggplot2)
library(openintro)
library(gtsummary)
library(caret)
library(pROC)
library(stargazer)
library(cvms)
library(gridExtra)
library(ggpubr)
library(purrr)
```

# 1. Introduction

## 1.1 Generalized Linear Models

Generalized Linear Models (GLMs) are a class of statistical models that extend linear regression to handle a broader range of response variable distributions such as binomial, Poisson, and gamma, making them suitable for diverse types of data. Unlike traditional linear regression, GLMs are not constrained by the assumption of normality.

The Primary components of a GLM are:

1.  **Response Variable**: with a distribution such as Binomial, Poisson etc.
2.  **Linear Predictor**: A Linear combination of the predictor variables (similar to Linear Regression)
3.  **Link Function**: to connect the linear predictor to the expected value of the response variable.

The General form of a GLM is:

$$
g(μ) = β_0 + β_1X_1 + β_2X_2 + ... + β_kX_k
$$

Where $g(\mu)$ is the link function, $\beta_i$ are the coefficients associated with the corresponding predictor variables $X_i$

## 1.2 Link Function

A link function is used to connect the linear predictor to the expected value of the response variable i.e. it describes how the mean of the response variable is related to a linear combination of the predictor variables. The link function, denoted as g($\mu$), transforms the linear predictor into a scale appropriate for the response variable. The choice of the link function depends on the nature of the response variable and the distribution assumed for it some examples of link funtions are **Logit, Inverse, Log** etc.

## 1.3 GLM: Multinomial Regression

Multinomial Regression is used when the response variable is categorical in nature and has more than 2 levels (categories). The link function used in Multinominal Regression is a **Logit** function which is defined as follows:

$$logit(p) = log(\frac{p}{1 - p})$$

some sample research questions which can be answered by multinomial regression include:

1.  What Occupation are people most likely to chose based on their parents occupation and their own education
2.  What food preferences will an animal have based on its size and habitat

# 2. Probability Distribution

## 2.1 Assumed Probability Distribution

Multinomial distribution assumes that the response variable has a multinomial distribution, which is a generalization of the binomial distribution for categorical variables with more than 2 categories.

The Probability Mass Function of a multinomial distribution is given by the equation:

$$Pr(X_1 = x_1 and ... and X_k = x_k) = \frac{n!}{x_1!...x_k!}p_1^{x_1}...p_k^{x_k}$$

where n is the total number of observations or trails, $x_i$ is the counts for each category and $p_i$ are the probabilities for each category.

## 2.2 Support

The **Support** for multinomial regression is given by:

$$
x_i\,\epsilon\{0,..,n\}, i\,\epsilon\{1,..,k\}, with\: \sum\limits_i x_i=n
$$

This implies that the number of times each outcome $x_i$ can occur is in the range 0 and the total number of observations n, and the sum of the count of all outcomes should add up to the total number of observations. k is the total number of possible categories of the outcome variable.

## 2.3 Parameters

The parameters for multinomial regression are that the number of trials "n" should be greater than 0 and the number of mutually exclusive events "k" should be greater than 0 with the probabilities of these events occurring $p_i$ taking values between 0 and 1 and sum of these probabilities should add up to 1 $(\sum\limits_i p_i=1)$

## 2.4 Example

Example if we try to predict the chances of picking a shape out of the circle, square and triangle and if we have 10 trials (n\>0),

the **support** would be that each of these shapes can be picked anywhere between 0 and 10 times but the sum of the number of times the shapes are picked should add upto 10 (e.g. Circle-3, Square-5, Triangle-2)

(assuming the example given above is the input data) the probabilities $p_{circle}=0.3,\: p_{square}=0.5,\: p_{triangle}=0.2$ would add up to 1.

# 3. Model

## 3.1 General Form

For computing the multinomial regression, one of the outcomes is set as the "reference" or "baseline" level and a logistic regression is performed between all the other levels with respect to the baseline (where j \>1) , the general equation of each of these is of the form:

$$
log(\frac{\pi_{ij}}{\pi_{i1}}) = β_{0j} + β_{1j}x_{i1}+...+ β_{pj}x_{ip}
$$

where $\pi_{i1}$ is the probability of the reference level being the outcome.

## 3.2 Link Function

As mentioned in Section-1, multinomial regression uses **Logit** as the link function. Logit function is used for the following reasons:

1.  Range of Probabilities: Logit ensures that the predicted probabilities are between 0 and 1 (Parameter requirements of selected Regression)
2.  Symmetry: The logit function is symmetric around 0.5, making it well-suited for modeling the odds of an event occurring
3.  Interpretability: The function provides coefficients that represent the log-odds. This makes the interpretation of coefficients more intuitive in terms of how the odds of being in a particular category change with changes in the predictor variables.

## 3.3 Assumptions

Multinomial Regression works on the following assumptions:

1.  Response Variable: The response variable is a categorical variable with multinomial distribution.
2.  Independence: The observations are independent of each other.
3.  Linearity: There is a linearly mapable (via link function) relationship between the predictor variables and the response variable

# 4. Sample Execution in R

## 4.1 Dataset

For the sample execution of a Multinominal GLM in R we will be using a **simulated** [Dataset](https://anlane611.github.io/ids702-fall23/DAA/DA3_Multinomial.csv). Since this is a simulated Dataset, the data is clean and has no missing values, so we will not be performing any cleaning of data.

The initial data has been read and stored in a variable called `df`. The dataset contains the following variables:

1.  `Y`- the multinomial dependent variable with three categories - 1, 2, and 3
2.  `X1`- a continuous predictor with values in the range 11.12 to 27.66
3.  `X2`- a categorical predictor with values 0 and 1

The following steps were performed on the dataset before using it for modelling:

1.  original dataset has a serial number column `X` which has been removed as it is not required.
2.  The Variables Y and X2 were converted to the correct datatypes (INT -\> Factor)

```{r}
#Loading the Data
df <- read.csv("https://anlane611.github.io/ids702-fall23/DAA/DA3_Multinomial.csv")

#Use Below load in case online load is not working
#df <- read.csv("../Resources/DA3_Multinomial.csv")
```

**Sample of Data**:

```{r, output=TRUE, fig.align='center'}
head(df,3)
```

```{r}
summary(df)
```

```{r}
#Remove the S.No column
df <- select(df, -X)
```

```{r}
df$Y <- as.factor(df$Y)
df$X2 <- as.factor(df$X2)
```

```{r, output=TRUE, fig.align='center'}
# Get summary statistics for the selected dataset
summary_df <- df %>%
  purrr::map_df(~summary(.x), .id = "Variable")


# Print the summary statistics as a table
knitr::kable(summary_df)
```

```{r}
str(df)
```

```{r}
plot_x1<- ggplot(df, aes(x = X1, fill = Y)) +
  geom_density(alpha = 0.5) +
  labs(x = "X1", y = "Density Distribution", fill = "Y")
plot_x1
```

```{r}
plot_x2 <- ggplot(df, aes(x=X2, fill=Y, label=after_stat(count)))+
  geom_bar(alpha = 0.5)+
  geom_text(stat='count',position=position_stack(vjust=0.5))
plot_x2
```

```{r, output = TRUE, fig.align="center"}
plot_3<- ggarrange(plot_x1, plot_x2,  nrow=1, common.legend = TRUE, legend="bottom")

annotate_figure(plot_3, top = text_grob("Distribution of Y w.r.t X1 and X2"))

```

## 4.2 Model Fitting

To fit the multinomial regrssion in R, we use the `multinom` function from the `nnet` package in R. the code to fit the model is as follows:

```{r, output=TRUE, echo=TRUE}
library(nnet)
df$Y2 <- relevel(df$Y, ref=1)
model <- multinom(Y2 ~X1+X2, data = df)
```

**Note**: Though it was not required, we created a new variable `Y2` in which we specify the level from Y which we want to use as reference level.

The values which are printed when the model is fit provide information about how the model was optimized (uses maximum likelihood)

## 4.3 Model Interpretation

We use the `summary` function to view the details of the model fit:

```{r, output=TRUE}
summary(model)
```

The interpratation of the model summary is as follows:

1.  Coefficients: These are the estimated parameters for the model. For example, the coefficient for X1 in category 2 is -0.2237668. This means for each one unit increase in X1, the log odds of Y being in category 2 versus the reference category (category 1) decrease by 0.2237668, holding all other variables constant.
2.  Std. Errors: These are the standard errors of the coefficients. They measure the variability in the estimate for the coefficient. Smaller standard errors mean the estimate is more precise.
3.  Residual Deviance: This is a measure of how well the model fits the data. Lower values indicate a better fit. In this case, the residual deviance is 639.7325.
4.  AIC: This is the Akaike Information Criterion, a measure of the relative quality of statistical models. Lower values indicate a better model. In this case, the AIC is 651.7325.

## 4.4 Model Assessment 

```{r}
head(model$fitted.values)
```

```{r}
head(predict(model))
```

```{r, output=TRUE}
confusionMatrix(predict(model), df$Y2, mode = "everything")
```

## 4.5 Model Results Visualization

To visualize the results of our model, we predict the values of our response variable(Y) on generated data for the independent variables (X1, X2) such that we cover their range of possible values.

We make the `predict` function generate the output as a probability so that it is easier for us to interpret.

```{r}
plot_df <- data.frame(X1=rep(seq(min(df$X1), max(df$X1), 0.1),2), X2=rep(c(0,1),166))
plot_df$X2 <- as.factor(plot_df$X2)
preds <- cbind(plot_df, predict(model, newdata = plot_df, type = "probs"))
head(preds)
```

```{r}
preds_long <- gather(preds, "level", "probability", 3:5)
preds_long$X2 <- as.factor(preds_long$X2)
head(preds_long)
```

```{r, output=TRUE, fig.align='center'}
ggplot(preds_long,aes(x=X1, y=probability,col=X2))+
  geom_line()+
  facet_grid(level~.)
```

The graph above shows the probabilities of Y being in the possible categories given the values of the independent variables. The probabilities (colored lines) will always add up to 1 for a given value of the independent variables.
